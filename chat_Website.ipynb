{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate,ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "import os\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\hp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "model=HuggingFaceEndpoint(repo_id=repo_id,huggingfacehub_api_token=api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Ins and Outs of Retrieval-Augmented Generation (RAG) | by TDS Editors | Towards Data ScienceOpen in appSign upSign inWriteSign upSign inThe Ins and Outs of Retrieval-Augmented Generation (RAG)TDS Editors·FollowPublished inTowards Data Science·Sent as aNewsletter·3 min read·Oct 12, 2023--ListenShareWhen accessible large language models first came on the scene, the excitement was impossible to miss: beyond their sheer novelty, they came with the promise to completely transform numerous fields and lines of work.Almost a year after the launch of ChatGPT, we’re far more aware of LLMs’ limitations, and of the challenges we face when we try to integrate them into real-world products. We’ve also, by now, come up with powerful strategies to complement and enhance LLMs’ potential; among these, retrieval-augmented generation (RAG) has emerged as—arguably—the most prominent. It gives practitioners the power to connect pre-trained models to external, up-to-date information sources that can generate more accurate and more useful outputs.This week, we’ve gathered a potent lineup of articles that explain the intricacies and practical considerations of working with RAG. Whether you’re deep in the ML trenches or approaching the topic from the perspective of a data scientist or product manager, gaining a deeper familiarity with this approach can help you prepare for whatever the future of AI tools brings. Let’s jump right in!Add Your Own Data to an LLM Using Retrieval-Augmented Generation (RAG)For a beginner-friendly introduction to the topic, Beatriz Stollnitz’s recent deep dive is a terrific resource to visit and bookmark for future reference. It goes through the theoretical foundations of RAG before transitioning to a hands-on basic implementation, showing how you can create a chatbot to help customers find information about the products a company sells.10 Ways to Improve the Performance of Retrieval Augmented Generation SystemsIf you’ve already started tinkering with RAG in your projects, you’ve likely observed that setting it up is one thing, but making it work consistently and produce the intended results is another: “RAG is easy to prototype, but very hard to productionize.” Matt Ambrogi’s guide provides pragmatic insights on bridging the gap between the framework’s potential and more tangible benefits.Photo by Frank Zhang on UnsplashRAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?There are more than a few alternatives to RAG when it comes to building better AI products. Heiko Hotz offers a nuanced and thorough comparison of RAG and model fine-tuning, another prominent strategy for upgrading the performance of generic LLMs. Ultimately, as Heiko eloquently puts it, “There is no one-size-fits-all solution; success lies in aligning the optimisation method with the specific requirements of the task.”For other excellent reads on topics ranging from counterfactual insights to dynamic pricing, we hope you explore some of our other recent highlights:If you’d like to test out the power of the ChatGPT API, Mariya Mansurova shares an introductory guide to using it for topic modeling.Looking to brush up on your programming skills? Marcin Kozak’s hands-on tutorial tackles NaN (not-a-number) values in Python and how to use them properly.Reza Bagheri is back with one of his trademark deep dives, this time covering the mathematical underpinnings of dimensionality (and the notorious “curse” thereof) in great detail.To learn about counterfactuals and their place within data analysis, don’t miss Maham Haroon’s clear and accessible explainer.Why are so many businesses jumping on the generative-AI bandwagon even in the absence of a well-defined business goal? Stephanie Kirmer digs into an emerging conundrum.After unpacking the potential of using a reinforcement-learning approach to dynamic pricing, Massimiliano Costacurta weighs the benefits of adding context to a multi-armed bandits solution.In a fun project walkthrough, Caroline Arnold shows how you can leverage pre-trained models and reanalysis data to create a custom AI weather-forecast app.Thank you for supporting our authors’ work! If you enjoy the articles you read on TDS, consider becoming a Medium member — it unlocks our entire archive (and every other post on Medium, too).Data ScienceThe VariableTds FeaturesTowards Data ScienceLarge Language Models----FollowWritten by TDS Editors67K Followers·Editor for Towards Data ScienceBuilding a vibrant data science and machine learning community. Share your insights and projects with our global audience: bit.ly/write-for-tdsFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://towardsdatascience.com/the-ins-and-outs-of-retrieval-augmented-generation-rag-56f470ccda4', 'title': 'The Ins and Outs of Retrieval-Augmented Generation (RAG) | by TDS Editors | Towards Data Science', 'description': 'When accessible large language models first came on the scene, the excitement was impossible to miss: beyond their sheer novelty, they came with the promise to completely transform numerous fields…', 'language': 'en'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://towardsdatascience.com/the-ins-and-outs-of-retrieval-augmented-generation-rag-56f470ccda4\")\n",
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe main themes of the provided set of documents revolve around:\\n\\n1. Machine Learning Community: The importance of collaboration, sharing insights, and publishing work within the machine learning community.\\n2. Writing for a Global Audience: Best practices for writing about machine learning topics in a way that is accessible to a global audience.\\n3. Projects and Applications: Sharing examples of machine learning projects and their applications, such as text-to-speech technology.\\n4. Privacy and Terms of Service: The role of privacy and terms of service in the machine learning field, particularly in relation to data usage.\\n5. Reinforcement Learning and Dynamic Pricing: The application of reinforcement learning, specifically the Multi-Armed Bandits algorithm, in the context of dynamic pricing.\\n6. Collaboration and Teamwork: Strategies for effective collaboration within machine learning teams or the broader field.\\n7. Continuous Learning: Emphasizing the importance of staying updated on the latest developments and best practices in machine learning.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_tempelate=\"\"\"This the following set of documents\n",
    "{docs}\n",
    "Based on this list of docs . please identify the main themes\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt=PromptTemplate.from_template(map_tempelate)\n",
    "map_chain=LLMChain(llm=model,prompt=map_prompt)\n",
    "\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt=PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain=LLMChain(llm=model,prompt=reduce_prompt)\n",
    "reduce_chain = LLMChain(llm=model, prompt=reduce_prompt)\n",
    "\n",
    "combine_document_chain=StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain,document_variable_name='docs'\n",
    ")\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_document_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_document_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=1500,\n",
    ")\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "split_docs=text_splitter.split_documents(docs)\n",
    "map_reduce_chain.run(split_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embeddings=HuggingFaceInferenceAPIEmbeddings(repo_id=repo_id , api_key=api,add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "import chromadb\n",
    "chroma_client=chromadb.Client()\n",
    "chroma_client.delete_collection('new_collection')\n",
    "collection=chroma_client.create_collection(name=\"new_collection\")\n",
    "vectorestore=Chroma.from_documents(documents=split_docs,\n",
    "                                   collection_name='new_collection',\n",
    "                                   embedding=embeddings)\n",
    "reteriver=vectorestore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"You are a helpful assistant that generates multiple search queries\n",
    "based on the sing queries realated to : {question} \\n\n",
    "Output (4 queries)\"\"\"\n",
    "prompt_rag_fusion=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries=(\n",
    "    prompt_rag_fusion\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split('\\n'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is RAG?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\dspy\\.venv\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "retrival_chain_rag_fusion= generate_queries | reteriver.map() | reciprocal_rank_fusion\n",
    "\n",
    "docs=retrival_chain_rag_fusion.invoke({'question':question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Retrieval-Augmented Generation (RAG) is a strategy that gives practitioners the power to connect pre-trained models to external, up-to-date information sources that can generate more accurate and more useful outputs. It is a tool that helps in boosting the performance of large language models (LLMs).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "\n",
    "template = \"\"\"Answer the following question based on the context:\n",
    "{context}\n",
    "Question:{question}\n",
    "if the question is  not related to context just simply say \"I am not trained on this topic\" \n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the final chain\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrival_chain_rag_fusion, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Now, use the invoke method correctly\n",
    "result = final_rag_chain.invoke({\"question\": question})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
